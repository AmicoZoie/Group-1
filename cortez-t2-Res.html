<head>
 <link rel="stylesheet" href="styles.css">
    </head>
    <body>
        <div class="pabigat">
            <h2>Results</h2><hr>
The results showed that GPs were highly receptive to the AI-generated content, stating they would accept 100% of the ChatGPT summaries in their practice compared to 92% of those written by junior doctors. When it came to detecting the author, the GPs achieved an overall accuracy of 60%, correctly identifying 13 AI summaries but also mislabeling eight doctor-written summaries as AI-generated. In terms of clinical quality, both groups performed exceptionally well, achieving a median adherence score of 19 out of 19 and a 97% mean adherence to the National Prescribing Centre minimum dataset, with no statistically significant difference between the two. Interestingly, the OpenAI Text Classifier failed entirely to identify the AI's work, labeling all ChatGPT summaries as "very unlikely" to be AI-generated, which suggests that the human GPs were more perceptive than the specialized detection software.        </div>
    </body>
</html>
